{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the quality of extracted content by comparing it with the human references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You try to use a model that was created with version 2.4.0.dev0, however, your version is 2.3.1. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n",
      "/Users/jimmy/miniforge3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint /Users/jimmy/.cache/huggingface/metrics/bleurt/BLEURT-20-D12/downloads/extracted/2b0bd60025f714bf0eca857470aa967f784a446243ab3666b88cb6794a07c374/BLEURT-20-D12.\n",
      "INFO:tensorflow:Config file found, reading.\n",
      "INFO:tensorflow:Will load checkpoint BLEURT-20-D12\n",
      "INFO:tensorflow:Loads full paths and checks that files exists.\n",
      "INFO:tensorflow:... name:BLEURT-20-D12\n",
      "INFO:tensorflow:... bert_config_file:bert_config.json\n",
      "INFO:tensorflow:... max_seq_length:512\n",
      "INFO:tensorflow:... vocab_file:None\n",
      "INFO:tensorflow:... do_lower_case:None\n",
      "INFO:tensorflow:... sp_model:sent_piece\n",
      "INFO:tensorflow:... dynamic_seq_length:True\n",
      "INFO:tensorflow:Creating BLEURT scorer.\n",
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n",
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n",
      "INFO:tensorflow:Will load model: /Users/jimmy/.cache/huggingface/metrics/bleurt/BLEURT-20-D12/downloads/extracted/2b0bd60025f714bf0eca857470aa967f784a446243ab3666b88cb6794a07c374/BLEURT-20-D12/sent_piece.model.\n",
      "INFO:tensorflow:SentencePiece tokenizer created.\n",
      "INFO:tensorflow:Creating Eager Mode predictor.\n",
      "INFO:tensorflow:Loading model.\n",
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n",
      "[nltk_data] Downloading package wordnet to /Users/jimmy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/jimmy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/jimmy/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import pandas as pd\n",
    "import jsonlines as jl\n",
    "\n",
    "from massw.metrics import compute_metrics, flatten_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspects = [\"context\", \"key_idea\", \"method\", \"outcome\", \"future_impact\"]\n",
    "annotation_path = \"../data/annotation_0531.jsonl\"\n",
    "prediction_models = [\"gpt4\", \"gpt35\", \"mixtral\"]\n",
    "prediction_paths = {model: f\"../data/{model}_0531.jsonl\" for model in prediction_models}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of annotation records: 240\n",
      "Length of predictions for gpt4: 120\n",
      "Length of predictions for gpt35: 120\n",
      "Length of predictions for mixtral: 120\n"
     ]
    }
   ],
   "source": [
    "predictions = {}\n",
    "\n",
    "with jl.open(annotation_path) as f:\n",
    "    annotations = []\n",
    "    for line in f:\n",
    "        annotations.append({\n",
    "            \"id\": line[\"id\"],\n",
    "            \"texts\": line[\"displayed_text\"],\n",
    "            \"context\": line[\"label_annotations\"][\"Multi-aspect Summary\"][\"Context\"],\n",
    "            \"key_idea\": line[\"label_annotations\"][\"Multi-aspect Summary\"][\"Key idea\"],\n",
    "            \"method\": line[\"label_annotations\"][\"Multi-aspect Summary\"][\"Method\"],\n",
    "            \"outcome\": line[\"label_annotations\"][\"Multi-aspect Summary\"][\"Outcome\"],\n",
    "            \"future_impact\": line[\"label_annotations\"][\"Multi-aspect Summary\"][\"Future Impact\"],\n",
    "        })\n",
    "\n",
    "for model in prediction_models:\n",
    "    prediction_path = prediction_paths[model]\n",
    "    with jl.open(prediction_path) as f:\n",
    "        predictions[model] = []\n",
    "        for line in f:\n",
    "            predictions[model].append({\n",
    "                \"id\": line[\"id\"],\n",
    "                \"context\": line[\"Context\"],\n",
    "                \"key_idea\": line[\"Key Idea\"],\n",
    "                \"method\": line[\"Method\"],\n",
    "                \"outcome\": line[\"Outcome\"],\n",
    "                \"future_impact\": line[\"Future Impact\"],\n",
    "            })\n",
    "\n",
    "print(\"Length of annotation records:\", len(annotations))\n",
    "for model in prediction_models:\n",
    "    print(f\"Length of predictions for {model}:\", len(predictions[model]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of annotation records after filtering: 240\n"
     ]
    }
   ],
   "source": [
    "# Preserve annotations labeled by at least 2 annotators\n",
    "id_counts = pd.Series([a[\"id\"] for a in annotations]).value_counts()\n",
    "annotations = [a for a in annotations if id_counts[a[\"id\"]] >= 2]\n",
    "print(\"Length of annotation records after filtering:\", len(annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of common records: 120\n"
     ]
    }
   ],
   "source": [
    "# Find intersection of annotations and predictions\n",
    "annotation_ids = set([a[\"id\"] for a in annotations])\n",
    "prediction_ids = {model: set([p[\"id\"] for p in predictions[model]]) for model in prediction_models}\n",
    "common_ids = annotation_ids\n",
    "for model in prediction_models:\n",
    "    common_ids = common_ids.intersection(prediction_ids[model])\n",
    "print(\"Number of common records:\", len(common_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics for gpt4\n",
      "Computing metrics for gpt4 on context\n",
      "Computing metrics for gpt4 on key_idea\n",
      "Computing metrics for gpt4 on method\n",
      "Computing metrics for gpt4 on outcome\n",
      "Computing metrics for gpt4 on future_impact\n",
      "Computing metrics for gpt35\n",
      "Computing metrics for gpt35 on context\n",
      "Computing metrics for gpt35 on key_idea\n",
      "Computing metrics for gpt35 on method\n",
      "Computing metrics for gpt35 on outcome\n",
      "Computing metrics for gpt35 on future_impact\n",
      "Computing metrics for mixtral\n",
      "Computing metrics for mixtral on context\n",
      "Computing metrics for mixtral on key_idea\n",
      "Computing metrics for mixtral on method\n",
      "Computing metrics for mixtral on outcome\n",
      "Computing metrics for mixtral on future_impact\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Compute all metrics\n",
    "# Model as prediction source, and annotation as ground truth references\n",
    "\n",
    "metrics = {}\n",
    "\n",
    "refs = {}\n",
    "for aspect in aspects:\n",
    "    refs[aspect] = []\n",
    "    for idx in common_ids:\n",
    "        refs[aspect].append([a[aspect] for a in annotations if a[\"id\"] == idx])\n",
    "\n",
    "for model in prediction_models:\n",
    "    print(f\"Computing metrics for {model}\")\n",
    "    metrics[model] = {}\n",
    "    model_predictions = {}\n",
    "    for aspect in aspects:\n",
    "        model_predictions[aspect] = []\n",
    "        for idx in common_ids:\n",
    "            found = [p[aspect] for p in predictions[model] if p[\"id\"] == idx]\n",
    "            assert len(found) == 1\n",
    "            model_predictions[aspect].append(found[0])\n",
    "\n",
    "    for aspect in aspects:\n",
    "        print(f\"Computing metrics for {model} on {aspect}\")\n",
    "        metrics[model][aspect] = compute_metrics(\n",
    "            predictions=model_predictions[aspect],\n",
    "            references=refs[aspect],\n",
    "            metric_names=[\"nahit\"]\n",
    "            # metric_names=[\"bleurt\", \"cosine\", \"bertscore\", \"rouge\", \"bleu\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# View metrics\n",
    "for model in prediction_models:\n",
    "    for aspect in aspects:\n",
    "        metrics[model][aspect] = flatten_metrics(metrics[model][aspect])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set float precision\n",
    "pd.set_option('display.precision', 3)\n",
    "gpt4_df = pd.DataFrame(metrics[\"gpt4\"]).rename(columns={\"future_impact\": \"projected_impact\"})\n",
    "gpt35_df = pd.DataFrame(metrics[\"gpt35\"]).rename(columns={\"future_impact\": \"projected_impact\"})\n",
    "mixtral_df = pd.DataFrame(metrics[\"mixtral\"]).rename(columns={\"future_impact\": \"projected_impact\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>key_idea</th>\n",
       "      <th>method</th>\n",
       "      <th>outcome</th>\n",
       "      <th>projected_impact</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>N/A-precision</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N/A-recall</th>\n",
       "      <td>0.583</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N/A-f1</th>\n",
       "      <td>0.737</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.457</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N/A in pred</th>\n",
       "      <td>0.117</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N/A in ref</th>\n",
       "      <td>0.200</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.867</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               context  key_idea  method  outcome  projected_impact\n",
       "N/A-precision    1.000     0.000   0.500    0.267             0.932\n",
       "N/A-recall       0.583     0.000   0.421    0.364             0.923\n",
       "N/A-f1           0.737     0.000   0.457    0.308             0.928\n",
       "N/A in pred      0.117     0.008   0.133    0.125             0.858\n",
       "N/A in ref       0.200     0.008   0.158    0.092             0.867"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>key_idea</th>\n",
       "      <th>method</th>\n",
       "      <th>outcome</th>\n",
       "      <th>projected_impact</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>N/A-precision</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.583</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N/A-recall</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N/A-f1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.609</td>\n",
       "      <td>0.514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N/A in pred</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N/A in ref</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.867</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               context  key_idea  method  outcome  projected_impact\n",
       "N/A-precision      0.0     0.000   1.000    0.583             1.000\n",
       "N/A-recall         0.0     0.000   0.105    0.636             0.346\n",
       "N/A-f1             0.0     0.000   0.190    0.609             0.514\n",
       "N/A in pred        0.0     0.000   0.017    0.100             0.300\n",
       "N/A in ref         0.2     0.008   0.158    0.092             0.867"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>key_idea</th>\n",
       "      <th>method</th>\n",
       "      <th>outcome</th>\n",
       "      <th>projected_impact</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>N/A-precision</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N/A-recall</th>\n",
       "      <td>0.042</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N/A-f1</th>\n",
       "      <td>0.080</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N/A in pred</th>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N/A in ref</th>\n",
       "      <td>0.200</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.867</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               context  key_idea  method  outcome  projected_impact\n",
       "N/A-precision    1.000     0.000   0.667    0.250             0.951\n",
       "N/A-recall       0.042     0.000   0.421    0.364             0.750\n",
       "N/A-f1           0.080     0.000   0.516    0.296             0.839\n",
       "N/A in pred      0.008     0.008   0.100    0.133             0.683\n",
       "N/A in ref       0.200     0.008   0.158    0.092             0.867"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(gpt4_df)\n",
    "display(gpt35_df)\n",
    "display(mixtral_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>key_idea</th>\n",
       "      <th>method</th>\n",
       "      <th>outcome</th>\n",
       "      <th>projected_impact</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Cosine Embedding</th>\n",
       "      <td>0.940</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.894</td>\n",
       "      <td>0.931</td>\n",
       "      <td>0.916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BLEURT</th>\n",
       "      <td>0.607</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BERTScore-f1</th>\n",
       "      <td>0.934</td>\n",
       "      <td>0.928</td>\n",
       "      <td>0.908</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BLEU</th>\n",
       "      <td>0.384</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROUGE-1</th>\n",
       "      <td>0.604</td>\n",
       "      <td>0.572</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  context  key_idea  method  outcome  projected_impact\n",
       "Cosine Embedding    0.940     0.944   0.894    0.931             0.916\n",
       "BLEURT              0.607     0.582   0.510    0.603             0.611\n",
       "BERTScore-f1        0.934     0.928   0.908    0.933             0.933\n",
       "BLEU                0.384     0.375   0.197    0.355             0.282\n",
       "ROUGE-1             0.604     0.572   0.450    0.596             0.563"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>key_idea</th>\n",
       "      <th>method</th>\n",
       "      <th>outcome</th>\n",
       "      <th>projected_impact</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Cosine Embedding</th>\n",
       "      <td>0.934</td>\n",
       "      <td>0.936</td>\n",
       "      <td>0.895</td>\n",
       "      <td>0.928</td>\n",
       "      <td>0.876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BLEURT</th>\n",
       "      <td>0.597</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BERTScore-f1</th>\n",
       "      <td>0.934</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.934</td>\n",
       "      <td>0.905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BLEU</th>\n",
       "      <td>0.524</td>\n",
       "      <td>0.439</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.452</td>\n",
       "      <td>0.170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROUGE-1</th>\n",
       "      <td>0.635</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.626</td>\n",
       "      <td>0.371</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  context  key_idea  method  outcome  projected_impact\n",
       "Cosine Embedding    0.934     0.936   0.895    0.928             0.876\n",
       "BLEURT              0.597     0.575   0.510    0.608             0.498\n",
       "BERTScore-f1        0.934     0.927   0.910    0.934             0.905\n",
       "BLEU                0.524     0.439   0.197    0.452             0.170\n",
       "ROUGE-1             0.635     0.582   0.445    0.626             0.371"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>key_idea</th>\n",
       "      <th>method</th>\n",
       "      <th>outcome</th>\n",
       "      <th>projected_impact</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Cosine Embedding</th>\n",
       "      <td>0.944</td>\n",
       "      <td>0.949</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BLEURT</th>\n",
       "      <td>0.645</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.554</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BERTScore-f1</th>\n",
       "      <td>0.946</td>\n",
       "      <td>0.943</td>\n",
       "      <td>0.920</td>\n",
       "      <td>0.948</td>\n",
       "      <td>0.936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BLEU</th>\n",
       "      <td>0.590</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.295</td>\n",
       "      <td>0.665</td>\n",
       "      <td>0.384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROUGE-1</th>\n",
       "      <td>0.693</td>\n",
       "      <td>0.662</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.707</td>\n",
       "      <td>0.599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  context  key_idea  method  outcome  projected_impact\n",
       "Cosine Embedding    0.944     0.949   0.905    0.933             0.917\n",
       "BLEURT              0.645     0.636   0.554    0.674             0.635\n",
       "BERTScore-f1        0.946     0.943   0.920    0.948             0.936\n",
       "BLEU                0.590     0.556   0.295    0.665             0.384\n",
       "ROUGE-1             0.693     0.662   0.509    0.707             0.599"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rows = [\"Cosine Embedding\", \"BLEURT\", \"BERTScore-f1\", \"BLEU\", \"ROUGE-1\"]\n",
    "# rows = [\"Cosine Embedding\", \"BLEURT\", \"BERTScore-f1\", \"BLEU\", \"ROUGE-1\", \"LLM Similarity\"]\n",
    "gpt4_df.loc[rows].to_csv(\"gpt4_quality.csv\", float_format=\"%.3f\")\n",
    "gpt35_df.loc[rows].to_csv(\"gpt35_quality.csv\", float_format=\"%.3f\")\n",
    "mixtral_df.loc[rows].to_csv(\"mixtral_quality.csv\", float_format=\"%.3f\")\n",
    "\n",
    "display(gpt4_df.loc[rows])\n",
    "display(gpt35_df.loc[rows])\n",
    "display(mixtral_df.loc[rows])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since every paper has two annotations, we can split the annotations into two\n",
    "# groups by their ids.\n",
    "annotations.sort(key=lambda x: x[\"id\"])\n",
    "annotations_1 = annotations[::2]\n",
    "annotations_2 = annotations[1::2]\n",
    "\n",
    "# Use one as reference and the other as prediction\n",
    "cross_val_metrics = {}\n",
    "for aspect in aspects:\n",
    "    cross_val_metrics[aspect] = flatten_metrics(\n",
    "        compute_metrics(\n",
    "            predictions=[a[aspect] for a in annotations_1],\n",
    "            references=[a[aspect] for a in annotations_2],\n",
    "            metric_names=[\"bleurt\", \"cosine\", \"bertscore\", \"rouge\", \"bleu\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>key_idea</th>\n",
       "      <th>method</th>\n",
       "      <th>outcome</th>\n",
       "      <th>projected_impact</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Cosine Embedding</th>\n",
       "      <td>0.935</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.936</td>\n",
       "      <td>0.941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BLEU</th>\n",
       "      <td>0.594</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.357</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision-1</th>\n",
       "      <td>0.694</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision-2</th>\n",
       "      <td>0.608</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.373</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Length Ratio</th>\n",
       "      <td>0.991</td>\n",
       "      <td>1.133</td>\n",
       "      <td>1.031</td>\n",
       "      <td>1.089</td>\n",
       "      <td>0.953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROUGE-1</th>\n",
       "      <td>0.703</td>\n",
       "      <td>0.637</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.737</td>\n",
       "      <td>0.748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROUGE-2</th>\n",
       "      <td>0.633</td>\n",
       "      <td>0.546</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.661</td>\n",
       "      <td>0.686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BERTScore-precision</th>\n",
       "      <td>0.942</td>\n",
       "      <td>0.934</td>\n",
       "      <td>0.922</td>\n",
       "      <td>0.947</td>\n",
       "      <td>0.959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BERTScore-recall</th>\n",
       "      <td>0.943</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.926</td>\n",
       "      <td>0.954</td>\n",
       "      <td>0.951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BERTScore-f1</th>\n",
       "      <td>0.942</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0.924</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BLEURT</th>\n",
       "      <td>0.656</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.559</td>\n",
       "      <td>0.671</td>\n",
       "      <td>0.742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     context  key_idea  method  outcome  projected_impact\n",
       "Cosine Embedding       0.935     0.944   0.900    0.936             0.941\n",
       "BLEU                   0.594     0.464   0.357    0.608             0.642\n",
       "Precision-1            0.694     0.563   0.524    0.699             0.757\n",
       "Precision-2            0.608     0.472   0.373    0.620             0.690\n",
       "Length Ratio           0.991     1.133   1.031    1.089             0.953\n",
       "ROUGE-1                0.703     0.637   0.540    0.737             0.748\n",
       "ROUGE-2                0.633     0.546   0.381    0.661             0.686\n",
       "BERTScore-precision    0.942     0.934   0.922    0.947             0.959\n",
       "BERTScore-recall       0.943     0.944   0.926    0.954             0.951\n",
       "BERTScore-f1           0.942     0.938   0.924    0.950             0.955\n",
       "BLEURT                 0.656     0.618   0.559    0.671             0.742"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_df = pd.DataFrame(cross_val_metrics).rename(columns={\"future_impact\": \"projected_impact\"})\n",
    "human_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>key_idea</th>\n",
       "      <th>method</th>\n",
       "      <th>outcome</th>\n",
       "      <th>projected_impact</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Cosine Embedding</th>\n",
       "      <td>0.935</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.936</td>\n",
       "      <td>0.941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BLEURT</th>\n",
       "      <td>0.656</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.559</td>\n",
       "      <td>0.671</td>\n",
       "      <td>0.742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BERTScore-f1</th>\n",
       "      <td>0.942</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0.924</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BLEU</th>\n",
       "      <td>0.594</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.357</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROUGE-1</th>\n",
       "      <td>0.703</td>\n",
       "      <td>0.637</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.737</td>\n",
       "      <td>0.748</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  context  key_idea  method  outcome  projected_impact\n",
       "Cosine Embedding    0.935     0.944   0.900    0.936             0.941\n",
       "BLEURT              0.656     0.618   0.559    0.671             0.742\n",
       "BERTScore-f1        0.942     0.938   0.924    0.950             0.955\n",
       "BLEU                0.594     0.464   0.357    0.608             0.642\n",
       "ROUGE-1             0.703     0.637   0.540    0.737             0.748"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "human_df.loc[rows].to_csv(\"human_agreement.csv\", float_format=\"%.3f\")\n",
    "\n",
    "display(human_df.loc[rows])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: ffd14676-a525-479f-a74e-2c5d3a85c510\n",
      "[gpt4]\n",
      "Context: Interest in parallel systems has been revived, focusing on computation through excitatory and inhibitory interactions in networks of neuron-like units, particularly for early stages of visual processing and the representation of small local fragments.\n",
      "Key Idea: The paper tackles the challenge of representing shapes in parallel systems and proposes mechanisms for shape perception and visual attention, offering a novel interpretation of the Gestalt principle 'the whole is more than the sum of its parts'.\n",
      "Method: N/A\n",
      "Outcome: N/A\n",
      "Future Impact: N/A\n",
      "[gpt35]\n",
      "Context: There has been a recent revival of interest in parallel systems in which computation is performed by excitatory and inhibitory interactions within a network of relatively simple, neuronlike units. This paper considers the difficulties involved in representing shapes in parallel systems.\n",
      "Key Idea: The authors suggest ways of representing shapes in parallel systems which provides a mechanism for shape perception and visual attention, and allows a novel interpretation of the Gestalt slogan.\n",
      "Method: N/A\n",
      "Outcome: N/A\n",
      "Future Impact: The proposed mechanism for shape perception and visual attention can be extended to other domain-specific problems involving the representation of complex shapes such as in medical imaging, robotics, and autonomous driving.\n",
      "[mixtral]\n",
      "Context: There has been a recent revival of interest in parallel systems in which computation is performed by excitatory and inhibitory interactions within a network of relatively simple, neuronlike units. Representing shapes in parallel systems is a complex issue.\n",
      "Key Idea: This paper considers the difficulties involved in representing shapes in parallel systems and suggests ways of overcoming them. It provides a mechanism for shape perception and visual attention.\n",
      "Method: N/A\n",
      "Outcome: N/A\n",
      "Future Impact: The paper's proposed mechanism for shape perception and visual attention can allow a novel interpretation of the Gestalt slogan that the whole is more than the sum of its parts.\n",
      "[Human]\n",
      "Context: In parallel systems, visual processing by having individual network units represent hypotheses about how small local fragments of the visual input should be interpreted and encode knowledge about the constraints between local interpretations. But the representational issues are more complex higher up in the visual system.\n",
      "Key Idea: The author provides a mechanism for shape perception and visual attention which allows a novel interpretation of the Gestalt slogan that the whole is more than the sum of its parts.\n",
      "Method: N/A\n",
      "Outcome: N/A\n",
      "Future Impact: N/A\n",
      "[Human]\n",
      "Context: There has been a recent revival of interest in parallel systems in which computation is performed by excitatory and inhibitory interactions within a network of relatively simple, neuronlike units. However, higher up than small local fragments, the representational issues are complex in the visual system.\n",
      "Key Idea: The authors suggest ways of overcoming the difficulties involved in representing shapes in parallel systems. They provide a mechanism for shape perception and visual attention.\n",
      "Method: N/A\n",
      "Outcome: The proposed mechanism allows a novel interpretation of the Gestalt slogan that the whole is more than the sum of its parts.\n",
      "Future Impact: N/A\n",
      "\n",
      "\n",
      "ID: 73e353a8-e0d6-466f-af93-6fccf38fcb18\n",
      "[gpt4]\n",
      "Context: Video-grounded dialogues require reasoning over dialogue context in a multi-turn setting, which is more complex than traditional visual question answering. Previous approaches mostly use dialogue context as simple text input without modelling the turn-level information flows.\n",
      "Key Idea: The proposed approach discovers information flows among dialogue turns through a semantic graph constructed based on lexical components in each question and answer, introducing a novel method to learn reasoning paths over this semantic graph.\n",
      "Method: The method involves predicting a reasoning path over the semantic graph from the current turn through past dialogue turns containing additional visual cues needed to answer the current question. The reasoning model processes both visual and textual information through this path, and the propagated features are used to generate the answer.\n",
      "Outcome: The experimental results demonstrate the effectiveness of the proposed method in video-grounded dialogues by showing how it efficiently uses semantic dependencies in dialogue to retrieve visual cues.\n",
      "Future Impact: The method provides insights on how models can leverage semantic dependencies in dialogues to retrieve visual information, suggesting potential applications in enhancing video-grounded dialogue systems.\n",
      "[gpt35]\n",
      "Context: Video-grounded dialogues require additional reasoning over dialogue context to answer questions in a multi-turn setting. Dialogue context is mostly used as a simple text input without modeling the inherent information flows at the turn level in previous approaches.\n",
      "Key Idea: The authors propose to discover information flows among dialogue turns through a semantic graph constructed based on lexical components in each question and answer, and introducing a new approach that learns to predict reasoning paths over this semantic graph.\n",
      "Method: The authors construct semantic graphs based on dialogue context, and propose a model that predicts reasoning paths through the graph. Additionally, their reasoning model processes both visual and textual information through this reasoning path and the propagated features are used to generate the answer.  \n",
      "Outcome: Experimental results demonstrate the effectiveness of the proposed method.\n",
      "Future Impact: N/A\n",
      "[mixtral]\n",
      "Context: Previous approaches to video-grounded dialogues mostly use dialogue context as a simple text input without modelling the inherent information flows at the turn level.\n",
      "Key Idea: The authors propose to discover information flows among dialogue turns through a semantic graph constructed based on lexical components in each question and answer. They then introduce a new approach that learns to predict reasoning paths over this semantic graph.\n",
      "Method: The authors construct a semantic graph based on lexical components in each question and answer. They then introduce a path prediction model that predicts a path from the current turn through past dialogue turns that contain additional visual cues to answer the current question. The reasoning model sequentially processes both visual and textual information through this reasoning path.\n",
      "Outcome: The experimental results demonstrate the effectiveness of the proposed method and provide insights on how models use semantic dependencies in a dialogue context to retrieve visual cues.\n",
      "Future Impact: N/A\n",
      "[Human]\n",
      "Context: Compared to traditional visual question answering, video-grounded dialogues require additional reasoning over dialogue context to answer questions in a multi-turn setting. Previous approaches to video-grounded dialogues mostly use dialogue context as a simple text input without modelling the inherent information flows at the turn level.\n",
      "Key Idea: The authors propose to introduce a new approach that learns to predict reasoning paths over this semantic graph for video-grounded dialogues.\n",
      "Method: N/A\n",
      "Outcome: The experimental results demonstrate the effectiveness of t he proposed method.\n",
      "Future Impact: N/A\n",
      "[Human]\n",
      "Context: Video-grounded dialogues require additional reasoning over dialogue context to answer questions in a multi-turn setting, but previous approaches to video-grounded dialogues mostly use dialogue context as a simple text input without modeling the inherent information flows at the turn level.\n",
      "Key Idea: The author introduces an approach that a path prediction model predicts a path from the current turn through past dialogue turns that contain additional visual cues to answer the current question, and than a reasoning model processes both visual and textual information through this reasoning path and the propagated features are used to generate the answer.\n",
      "Method: N/A\n",
      "Outcome: The proposed method is demonstrated as effective, and insights on how models use semantic dependencies in a dialogue context to retrieve visual cues is are provided.\n",
      "Future Impact: N/A\n",
      "\n",
      "\n",
      "ID: 14b0ebd1-b654-4eed-bdd8-ebeb74250b15\n",
      "[gpt4]\n",
      "Context: In few-shot relational triple extraction (FS-RTE), traditional approaches follow an entity-then-relation paradigm which first extracts all entities and then classifies their relations, overlooking the entity discrepancy between different relations.\n",
      "Key Idea: The authors propose a novel task decomposition strategy for FS-RTE called Relation-then-Entity, which first detects the relations in a sentence and then extracts the corresponding head/tail entities of these relations. A model named RelATE is further introduced to implement this strategy.\n",
      "Method: RelATE uses a dual-level attention mechanism to aggregate relation-relevant information for detecting relation occurrences and utilizes the annotated samples of the detected relations for extracting the corresponding head/tail entities.\n",
      "Outcome: The RelATE model significantly outperforms previous works in FS-RTE, showcasing an absolute gain of 18.98% and 28.85% in F1 in two different few-shot settings.\n",
      "Future Impact: N/A\n",
      "[gpt35]\n",
      "Context: In the few-shot relational triple extraction (FS-RTE), one extracts relational triples from plain texts by utilizing only few annotated samples. The entity-then-relation paradigm, which first extracts all entities and then classifies their relations, ignores the entity discrepancy between relations.\n",
      "Key Idea: The authors propose a novel task decomposition strategy, Relation-then- Entity, for FS-RTE and a model, RelATE to instantiate this strategy. RelATE builds a dual-level attention to aggregate relation-relevant information to detect the relation occurrence and utilizes the annotated samples of the detected relations to extract the corresponding head/tail entities.\n",
      "Method: The authors experimentally show that RelATE outperforms previous work by an absolute gain (18.98%, 28.85% in F1 in two few-shot settings).\n",
      "Outcome: RelATE outperforms previous models with an absolute improvement of 18.98% and 28.85% F1 scores.\n",
      "Future Impact: N/A\n",
      "[mixtral]\n",
      "Context: In few-shot relational triple extraction (FS-RTE), the goal is to extract relational triples from plain texts using only a few annotated samples. Previous work extracts all entities and then classifies their relations, which ignores the entity discrepancy between relations.\n",
      "Key Idea: The authors propose a novel task decomposition strategy called Relation-then-Entity (RelATE) for FS-RTE. It first detects relations in a sentence and then extracts the corresponding head/tail entities of the detected relations.\n",
      "Method: The authors propose a model called RelATE that builds a dual-level attention to aggregate relation-relevant information to detect the relation occurrence and uses the annotated samples of the detected relations to extract the corresponding head/tail entities.\n",
      "Outcome: The proposed model outperforms previous work by an absolute gain of 18.98% and 28.85% in F1 in two few-shot settings.\n",
      "Future Impact: N/A\n",
      "[Human]\n",
      "Context: In few-shot relational triple extraction, one seeks to extract relational triples from plain texts by utilizing few annotated samples. Recent work first extracts all entities and then classifies their relations, which ignores the entity discrepancy between relations.\n",
      "Key Idea: The author first proposes a novel task decomposition strategy for FS-RTE, which detects relations occurred in a sentence and then extracts the corresponding head/tail entities of the detected relations. To instantiate this strategy, the author also proposes a model that builds a dual-level attention to aggregate relation-relevant information to detect the relation occurrence and utilizes the annotated samples of the detected relations to extract the corresponding head/tail entities.\n",
      "Method: The author assesses the performance of the proposed strategy against previous works.\n",
      "Outcome: Experimental results show that the proposed model outperforms previous work by an absolute gain (18.98%, 28.85% in F1 in two few-shot settings).\n",
      "Future Impact: N/A\n",
      "[Human]\n",
      "Context: Recent work in few-shot relational triple extraction (FS-RTE) first extracts all entities and then classifies their relations. Such an entity-then-relation paradigm ignores the entity discrepancy between relations.\n",
      "Key Idea: The authors propose a novel task decomposition strategy, Relation-then-Entity, for FS-RTE, which first detects relations occurred in a sentence and then extracts the corresponding head/tail entities of the detected relations. They further propose a model, RelATE, which builds a dual-level attention to aggregate relation-relevant information to detect the relation occurrence and utilizes the annotated samples of the detected relations to extract the corresponding head/tail entities.\n",
      "Method: N/A\n",
      "Outcome: Experimental results show that our model outperforms previous work by an absolute gain (18.98%, 28.85% in F1 in two few-shot settings).\n",
      "Future Impact: N/A\n",
      "\n",
      "\n",
      "ID: ca53b2c4-2912-4515-aae6-938c3f268a60\n",
      "[gpt4]\n",
      "Context: The problem of detecting epidemic tendency by mining search logs.\n",
      "Key Idea: The key idea is an algorithm that selects epidemic related queries/terms based on click-through information and adopts linear regression to model epidemic occurrences and frequencies of epidemic related terms in search logs.\n",
      "Method: The effectiveness of the algorithm is tested by evaluating the correlation between the frequencies of epidemic related terms found in search logs and actual epidemic occurrences.\n",
      "Outcome: The results demonstrate that the algorithm effectively identifies epidemic related terms that show a high correlation with epidemic occurrences. It also shows that combining different epidemic related terms improves performance compared to using a single term.\n",
      "Future Impact: N/A\n",
      "[gpt35]\n",
      "Context: The authors address the problem of detecting epidemic tendency by mining search logs.\n",
      "Key Idea: The authors propose an algorithm to select epidemic related queries/terms by using click-through information. They adopt linear regression to model epidemic occurrences and frequencies of epidemic related terms in search logs.\n",
      "Method: The authors use click-through information to select epidemic-related terms, followed by linear regression to model epidemic occurrences and the frequencies of epidemic-related terms in search logs. They compare the performance of combining different epidemic-related terms against using a single epidemic-related term.\n",
      "Outcome: The proposed algorithm is effective in identifying ERTs that show a high correlation with epidemic occurrences. The authors show that combining different ERTs performs better than using a single ERT.\n",
      "Future Impact: N/A\n",
      "[mixtral]\n",
      "Context: The problem of detecting epidemic tendency by mining search logs, and the need for an algorithm that can select epidemic related queries/terms.\n",
      "Key Idea: The authors propose an algorithm that uses click-through information to select epidemic related queries/terms, and a method to model epidemic occurrences and frequencies of epidemic related terms (ERTs) in search logs using linear regression.\n",
      "Method: The authors perform experiments to test the effectiveness of the algorithm in finding ERTs which obtain a high correlation value with epidemic occurrences.\n",
      "Outcome: The proposed algorithm is effective in finding ERTs which obtain a high correlation value with epidemic occurrences. The proposed method performs better when combining different ERTs than using single ERT.\n",
      "Future Impact: N/A\n",
      "[Human]\n",
      "Context: N/A\n",
      "Key Idea: The authors propose an algorithm based on click-through information to select epidemic related queries/terms, and adopt linear regression to model epidemic occurrences and frequencies of epidemic related terms (ERTs) in search log.\n",
      "Method: N/A\n",
      "Outcome: The results show the proposed algorithm is effective in finding ERTs which obtain a high correlation value with epidemic occurrences. The proposed method performs better when combining different ERTs than using single ERT.\n",
      "Future Impact: N/A\n",
      "[Human]\n",
      "Context: The problem of detecting epidemic tendency is important.\n",
      "Key Idea: Authors propose an algorithm based on click-through information to select epidemic related queries/terms. Authors adopt linear regression to model epidemic occurrences and frequencies of epidemic related terms (ERTs) in search logs.\n",
      "Method: Authors design experiments to find epidemic related terms (ERTs).\n",
      "Outcome: Experimental results show the proposed algorithm is effective in finding ERTs which obtain a high correlation value with epidemic occurrences. Authors also find the proposed method performs better when combining different ERTs than using single ERT.\n",
      "Future Impact: N/A\n",
      "\n",
      "\n",
      "ID: 5e1f387c-d883-4d1f-8397-e4a533a3387b\n",
      "[gpt4]\n",
      "Context: Hyperspectral imaging is crucial for various applications but is hindered by degradation issues and the scarcity of accurate ground-truth signals, making the training of deep neural networks for restoration tasks challenging.\n",
      "Key Idea: The paper introduces a hybrid approach based on sparse coding principles that combines the interpretability of classical techniques with the ability to train model parameters end-to-end without large datasets.\n",
      "Method: The efficacy of the proposed method is demonstrated through various denoising benchmarks.\n",
      "Outcome: The proposed method is computationally efficient and significantly outperforms state-of-the-art approaches in hyperspectral image restoration.\n",
      "Future Impact: N/A\n",
      "[gpt35]\n",
      "Context: Hyperspectral imaging suffers from various sources of degradations, and lack of accurate ground-truth 'clean' hyperspectral signals makes restoration tasks challenging. Traditional neural networks struggle to train correctly due to limited available data.\n",
      "Key Idea: The authors propose a new approach to hyperspectral image restoration based on sparse coding principles, merging classical techniques with deep learning to allow training of parameters without significant data requirements.\n",
      "Method: The proposed spectral-spatial sparse coding model is applied to various denoising benchmarks and is shown to be computationally efficient while significantly outperforming the current state of the art.\n",
      "Outcome: The proposed spectral-spatial sparse coding model is shown to be highly effective in improving denoising benchmarks.\n",
      "Future Impact: The hybrid approach based on sparse coding principles proposed in this paper is a promising avenue for future research in hyperspectral imaging restoration and may be useful for other applications with limited training data.\n",
      "[mixtral]\n",
      "Context: Hyperspectral imaging is used in various fields but it is affected by different sources of degradation, and the lack of accurate ground-truth makes restoration tasks challenging. Training deep neural networks for restoration is difficult in contrast to traditional RGB imaging problems.\n",
      "Key Idea: The authors propose a hybrid approach based on sparse coding principles that retain the interpretability of classical techniques encoding domain knowledge with handcrafted image priors, while allowing to train model parameters end-to-end without massive amounts of data.\n",
      "Method: The authors perform experiments on various denoising benchmarks to show the computational efficiency and the performance of the proposed method.\n",
      "Outcome: The proposed method is computationally efficient and significantly outperforms the state of the art on various denoising benchmarks.\n",
      "Future Impact: N/A\n",
      "[Human]\n",
      "Context: Hyperspectral imaging offers new perspectives for diverse applications, but the spectral diversity of information comes at the expense of various sources of degradation, and the lack of accurate ground-truth \"clean\" hyperspectral signals acquired on the spot makes restoration tasks challenging.\n",
      "Key Idea: The authors propose a hybrid approach based on sparse coding principles that retain the interpretability of classical techniques encoding domain knowledge with handcrafted image priors, while allowing to train model parameters end-to-end without massive amounts of data.\n",
      "Method: The authors evaluate their proposed method on various denoising benchmarks.\n",
      "Outcome: The authors show on various denoising benchmarks that the proposed method is computationally efficient and significantly outperforms the state of the art.\n",
      "Future Impact: N/A\n",
      "[Human]\n",
      "Context: Hyperspectral imaging offers new perspectives for diverse applications. Unfortunately, the spectral diversity of information comes at the expense of various sources of degradation, and the lack of accurate ground-truth \"clean\" hyperspectral signals acquired on the spot makes restoration tasks challenging. In particular, training deep neural networks for restoration is difficult, in contrast to traditional RGB imaging problems.\n",
      "Key Idea: Authors advocate instead for a hybrid approach based on sparse coding principles that retain the interpretability of classical techniques encoding domain knowledge with handcrafted image priors, while allowing to train model parameters end-to-end without massive amounts of data.\n",
      "Method: Authors conduct experiments on various denoising benchmarks.\n",
      "Outcome: Authors show that, on various denoising benchmarks the proposed method is computationally efficient and significantly outperforms the state of the art.\n",
      "Future Impact: N/A\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load a few examples for manual inspection\n",
    "ids = list(common_ids)[:5]\n",
    "for idx in ids:\n",
    "    print(\"ID:\", idx)\n",
    "    for model in prediction_models:\n",
    "        found = [p for p in predictions[model] if p[\"id\"] == idx]\n",
    "        assert len(found) == 1\n",
    "        print(f\"[{model}]\")\n",
    "        print(\"Context:\", found[0][\"context\"])\n",
    "        print(\"Key Idea:\", found[0][\"key_idea\"])\n",
    "        print(\"Method:\", found[0][\"method\"])\n",
    "        print(\"Outcome:\", found[0][\"outcome\"])\n",
    "        print(\"Future Impact:\", found[0][\"future_impact\"])\n",
    "    human_annotations = [a for a in annotations if a[\"id\"] == idx]\n",
    "    for a in human_annotations:\n",
    "        print(\"[Human]\")\n",
    "        print(\"Context:\", a[\"context\"])\n",
    "        print(\"Key Idea:\", a[\"key_idea\"])\n",
    "        print(\"Method:\", a[\"method\"])\n",
    "        print(\"Outcome:\", a[\"outcome\"])\n",
    "        print(\"Future Impact:\", a[\"future_impact\"])\n",
    "\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
