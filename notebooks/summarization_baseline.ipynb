{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240\n"
     ]
    }
   ],
   "source": [
    "from massw import api_gpt as api\n",
    "import pandas as pd\n",
    "import jsonlines as jl\n",
    "import json\n",
    "\n",
    "with jl.open(\"../data/annotation_0531.jsonl\") as f:\n",
    "    annotated_instances = list(f)\n",
    "print(len(annotated_instances))\n",
    "# Process the annotated instances\n",
    "df_annotated = pd.DataFrame(annotated_instances)\n",
    "df_annotated.set_index(\"id\", inplace=True)\n",
    "df_annotated[\"Context\"] = df_annotated[\"label_annotations\"].apply(lambda x: x[\"Multi-aspect Summary\"][\"Context\"])\n",
    "df_annotated[\"Key Idea\"] = df_annotated[\"label_annotations\"].apply(lambda x: x[\"Multi-aspect Summary\"][\"Key idea\"])\n",
    "df_annotated[\"Method\"] = df_annotated[\"label_annotations\"].apply(lambda x: x[\"Multi-aspect Summary\"][\"Method\"])\n",
    "df_annotated[\"Outcome\"] = df_annotated[\"label_annotations\"].apply(lambda x: x[\"Multi-aspect Summary\"][\"Outcome\"])\n",
    "df_annotated[\"Future Impact\"] = df_annotated[\"label_annotations\"].apply(lambda x: x[\"Multi-aspect Summary\"][\"Future Impact\"])\n",
    "df_annotated.drop(columns=[\"label_annotations\", \"span_annotations\"], inplace=True)\n",
    "ids = df_annotated.index.unique()\n",
    "texts = {id: df_annotated.reset_index().drop_duplicates(\"id\").set_index(\"id\").loc[id, \"displayed_text\"] for id in ids}\n",
    "azure_config = api.AzureConfig()\n",
    "batch = api.Batch(tpm=40000, azure=azure_config)\n",
    "\n",
    "with open(\"prompt.txt\", \"r\") as f:\n",
    "    system_prompt = f.read()\n",
    "\n",
    "example_prompt = \"\"\"\n",
    "Title: Attention Is All You Need\n",
    "Abstract: The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n",
    "\"\"\"\n",
    "\n",
    "example_output = \"\"\"{\n",
    "  \"Context\": \"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing sequence transduction models connect the encoder and decoder through an attention mechanism.\",\n",
    "  \"Key Idea\": \"The authors propose a simple network architecture called Transformer based solely on attention mechanisms and dispenses with recurrence and convolutions.\",\n",
    "  \"Method\": \"The authors perform experiments on the WMT 2014 English-to-German and English-to-French translation task. The authors apply the proposed model to English constituency parsing both with large and limited training data.\",\n",
    "  \"Outcome\": \"The proposed model achieves a BLEU score of 28.4 on the WMT 2014 English-to-French translation task. The proposed model achieves a BLEU score of 41.8 on the WMT 2014 English-to-German translation task after training for 3.5 days on 8 GPUs.\",\n",
    "  \"Future Impact\": \"N/A\"\n",
    "}\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in ids:\n",
    "    await batch.add(\n",
    "        \"chat.completions.create\",\n",
    "        model=\"gpt-35-turbo\",\n",
    "        # response_format={ \"type\": \"json_object\" },\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": example_prompt},\n",
    "            {\"role\": \"assistant\", \"content\": example_output},\n",
    "            {\"role\": \"user\", \"content\": texts[i]},\n",
    "        ],\n",
    "        metadata={\"id\": i},\n",
    "    )\n",
    "\n",
    "results = await batch.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9821d5f7-72b0-4841-a54f-d2af4a04ea3a\n",
      "{\n",
      "  \"Context\": \"The paper addresses the problem of inverse rendering where estimating the spherical harmonic illumination coefficients and texture parameters in a specular invariant colour subspace is challenging.\",\n",
      "  \"Key Idea\": \"The authors propose a novel approach for inverse rendering based on a linear basis approximation of surface texture, which can account for non-Lambertian specular reflectance and complex illumination of the same light source colour.\",\n",
      "  \"Method\": \"The proposed approach involves fitting a 3D morphable model to a single colour image of faces through the solution of bilinear equations in a specular invariant colour subspace. This approach recovers statistical texture model parameters without relying on computationally expensive analysis-by-synthesis techniques.\",\n",
      "  \"Outcome\": \"The proposed approach recovers texture model parameters with an accuracy comparable to that of more computationally expensive methods, while requiring only the solution of convex optimization problems.\",\n",
      "  \"Future Impact\": \"The proposed approach could be extended to other objects besides faces, potentially offering a more efficient and accurate solution to the problem of inverse rendering.\",\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# id2result\n",
    "id2result = {}\n",
    "for _, result in results.iterrows():\n",
    "    content = result[\"result\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "    # Remove \"\"\"json and \"\"\" if present\n",
    "    content = content.replace('\"\"\"json', \"\").replace('\"\"\"', \"\")\n",
    "    try:\n",
    "        id2result[result[\"id\"]] = json.loads(content)\n",
    "    except Exception as e:\n",
    "        print(result[\"id\"])\n",
    "        print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with jl.open(\"../data/gpt35_0531.jsonl\", \"w\") as f:\n",
    "    f.write_all([{\"id\": k, **v} for k, v in id2result.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixtral baselines\n",
    "import ssl\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "\n",
    "def allowSelfSignedHttps(allowed):\n",
    "    # Bypass the server certificate verification on client side\n",
    "    if allowed and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(\n",
    "            ssl, '_create_unverified_context', None):\n",
    "        ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "\n",
    "# Enabling self-signed certificates if required\n",
    "allowSelfSignedHttps(True)\n",
    "\n",
    "\n",
    "def mixtral_request(messages):\n",
    "    data = {\n",
    "        \"input_data\": {\n",
    "            \"input_string\": messages,\n",
    "            \"parameters\": {\n",
    "                \"temperature\": 0.6,\n",
    "                \"top_p\": 0.9,\n",
    "                \"do_sample\": True,\n",
    "                \"max_new_tokens\": 500,\n",
    "                \"return_full_text\": True\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    url = 'https://xingjian-ml-apqyj.eastus.inference.ml.azure.com/score'\n",
    "    api_key = '46g9IOYuQYjwCjlHzwEy9lBOJfQHrjZO'  # Mixtral API key\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': ('Bearer ' + api_key),\n",
    "        'azureml-model-deployment': 'mixtralai-mixtral-8x7b-instru-7'\n",
    "    }\n",
    "    body = str.encode(json.dumps(data))\n",
    "\n",
    "    req = urllib.request.Request(url, body, headers)\n",
    "    try:\n",
    "        response = urllib.request.urlopen(req)\n",
    "        result = response.read()\n",
    "        response = json.loads(result)[\"output\"]\n",
    "    except urllib.error.HTTPError as error:\n",
    "        print(\"The request failed with status code: \" + str(error.code))\n",
    "\n",
    "        # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n",
    "        print(error.info())\n",
    "        print(error.read().decode(\"utf8\", 'ignore'))\n",
    "    return response\n",
    "\n",
    "\n",
    "def get_mixtral_result(text):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": system_prompt},\n",
    "        {\"role\": \"assistant\", \"content\": \"I understand. Please give me futher information.\"},\n",
    "        {\"role\": \"user\", \"content\": example_prompt},\n",
    "        {\"role\": \"assistant\", \"content\": example_output},\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ]\n",
    "    return mixtral_request(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa07372be3eb44aaab1ac4d135c58645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "id2result_mixtral = {}\n",
    "for i in tqdm(ids):\n",
    "    id2result_mixtral[i] = get_mixtral_result(texts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      " {\n",
      "  \"Context\": \"Non-pharmacological interventions, such as reminiscence and biographical cognitive stimulation practices, are common and effective for people with dementia. However, obtaining and maintaining biographical or personalized materials can be challenging.\",\n",
      "  \"Key Idea\": \"The authors created a web platform that supports the work of psychologists in collecting and managing biographical materials for use in reminiscence and other biographical cognitive stimulation practices.\",\n",
      "  \"Method\": \"The authors conducted a case study with one psychologist and three patients, using the platform for a period of two weeks.\",\n",
      "  \"Outcome\": \"The results of the case study showed improvements in the collection of meaningful data about a person and in maintaining awareness of the therapy as a whole.\",\n",
      "  \"Future Impact\": \"The platform has the potential to be widely adopted in the field of dementia care, improving the quality and efficiency of non-pharmacological interventions.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(len(id2result_mixtral))\n",
    "print(list(id2result_mixtral.values())[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adfd8058-64b3-4062-953c-034b732e2fa0\n",
      " {\n",
      "\"Context\": \"In video compression, P and B-frames are typically coded with complex recurrent or convolutional neural networks, while I-frames are coded with H.2\n"
     ]
    }
   ],
   "source": [
    "def parse_json_garbage(s):\n",
    "    s = s[next(idx for idx, c in enumerate(s) if c in \"{[\"):]\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except json.JSONDecodeError as e:\n",
    "        return json.loads(s[:e.pos])\n",
    "\n",
    "id2json_mixtral = {}\n",
    "for k, v in id2result_mixtral.items():\n",
    "    try:\n",
    "        id2json_mixtral[k] = parse_json_garbage(v)\n",
    "    except Exception as e:\n",
    "        print(k)\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with jl.open(\"../data/mixtral_0531.jsonl\", \"w\") as f:\n",
    "    f.write_all([{\"id\": k, **v} for k, v in id2json_mixtral.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' {\\n\"Context\": \"Current video compression schemes are based on complex algorithms such as H.264, which may not be efficient in coding certain types of video sequences.\",\\n\"Key Idea\": \"The authors propose a video compression scheme based on texture synthesis through Directional Empirical Mode Decomposition (DEMD) algorithm. The proposed scheme decompose P and B-frames into Intrinsic Mode Function (IMF) image and its residue, and only the first level IMF image for P and B frames are coded.\",\\n\"Method\": \"The authors perform wavelet decomposition over residual image and use energy level at the HH band as a decision criterion for number of decomposition to be performed for optimum synthesis. The authors also demonstrate the effectiveness of the algorithm in multi-resolution parametric modeling of image data and scalable coding of IMF parameters.\",\\n\"Outcome\": \"The proposed scheme demonstrates significant compression with acceptable quality.\",\\n\"Future Impact\": \"N/A\"\\n}'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fix ad hoc\n",
    "get_mixtral_result(texts[\"adfd8058-64b3-4062-953c-034b732e2fa0\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
